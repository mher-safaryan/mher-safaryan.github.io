<!DOCTYPE HTML>
<!--
	Read Only by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Mher Safaryan</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Header -->
			<section id="header">
				<header>
					<span class="image avatar"><img src="images/avatar.jpg" alt="" /></span>
					<h1 id="logo">Mher Safaryan</h1>
					<p> Assistant Professor (UK Lecturer) <br /> Lancaster University</p>
				</header>
				<div>
					<ul class="icons">
						<li><a href="https://scholar.google.com/citations?user=dJNwgT8AAAAJ&hl=en" class="ai ai-google-scholar"><span class="label"></span></a></li>
						<li><a href="https://www.semanticscholar.org/author/M.-Safaryan/144033132" class="ai ai-semantic-scholar"><span class="label"></span></a></li>
						<li><a href="https://orcid.org/0000-0001-6290-1398" class="ai ai-orcid"><span class="label"></span></a></li>
						<li><a href="https://www.linkedin.com/in/mher-safaryan-94565a257/" class="icon brands fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
						<!-- <li><a href="images/cv.pdf" class="ai ai-cv"><span class="label"></span></a></li> -->
						<li><a href="mailto:mher.safaryan@ist.ac.at" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
					</ul>
				</div>
				<nav id="nav">
					<ul>
						<li><a href="#one" class="active">Bio</a></li>
						<li><a href="#two">Research</a></li>
						<li><a href="#three">News</a></li>
						<li><a href="#four">Contacts</a></li>
					</ul>
				</nav>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">

						<!-- One -->
							<section id="one">
								<!-- <div class="image main" data-position="center">
									<img src="images/banner.jpg" alt="" />
								</div> -->
								<div class="container">
									<header class="major">
										<h2>Bio</h2>
										<!-- <p>Just an incredibly simple responsive site<br />
										template freebie by <a href="http://html5up.net">HTML5 UP</a>.</p> -->
										<!-- <a href=""></a> -->
									</header>
									<p>
										I am an Assistant Professor (UK Lecturer) in the <a href="https://www.lancaster.ac.uk/maths/">School of Mathematical Sciences</a> at <a href="https://www.lancaster.ac.uk">Lancaster University</a>, within the <a href="https://www.lancaster.ac.uk/maths/research/mars/">MARS (Mathematics for AI in Real-world Systems)</a> section.
                                    <p/>
									<p>
										I was a postdoctoral researcher at the <a href="https://www.ist.ac.at">Institute of Science and Technology Austria (ISTA)</a>, working in the group led by <a href="https://daslab.ista.ac.at/">Prof. Dan Alistarh</a>. I was awarded a Marie Skłodowska-Curie Fellowship through the MSCA COFUND IST-BRIDGE program at ISTA. As part of the fellowship, I had the opportunity to work with <a href="https://www.linkedin.com/in/marquesan?original_referer=https%3A%2F%2Fwww.google.com%2F">Dr. Alexandre Marques</a> during an industrial secondment at <a href="https://neuralmagic.com/">Neural Magic, Inc.</a> (now acquired by Red Hat) in the USA.
									<!-- <p/>
									<p> -->
										Before joining ISTA in 2022, I was a postdoctoral research fellow at <a href="https://cemse.kaust.edu.sa">King Abdullah University of Science and Technology (KAUST)</a> in Saudi Arabia from 2019 to 2022, in the group led by <a href="https://richtarik.org">Prof. Peter Richtárik</a>. Prior to that, I worked with <a href="https://academia.kaust.edu.sa/en/persons/diogo.gomes">Prof. Diogo Gomes</a> at KAUST as a research technician from 2016 to 2019.
									<!-- <p/>
									<p> -->
										I obtained my Ph.D. in Mathematics in 2018 from <a href="http://ysu.am/main/en">Yerevan State University (YSU)</a> in Armenia, under the supervision of <a href="http://math.sci.am/user/grigori-karagulyan">Prof. Grigori Karagulyan</a>.
									</p>

                                    <p>
                                    	<span style='font-weight: bold; font-size: 13pt;'>Open Positions:</span> Our section <a href="https://www.lancaster.ac.uk/maths/research/mars/">MARS (Mathematics for AI in Real-world Systems)</a> is currently recruiting <a href="https://www.lancaster.ac.uk/maths/research/mars/phds/apply-to-mars/"><span style='font-weight: bold'>PhD positions</span></a> for students with UK fee status (deadline: 31 January 2026). You can find the list of pojects <a href="https://www.lancaster.ac.uk/maths/research/mars/phds/mars-indicative-phd-projects/">here</a>, including my project on "Large-Scale Optimization for Machine Learning."
                                        We also have <a href="https://hr-jobs.lancs.ac.uk/Vacancy.aspx?ref=0937-25"><span style='font-weight: bold'>Senior Research Associate positions</span></a>, offering the opportunity to lead your own independent three-year research project within the MARS programme (deadline: 4 January 2026). Feel free to reach out if you have questions or are interested in applying!
                                    </p>

									<p>
										<h3>Research Interests</h3>
									<ul class="research-interests">
										<li>optimization (theory and algorithms), machine learning, federated learning</li>
										<li>large-scale, convex/non-convex, stochastic/deterministic optimization, variance reduction</li>
										<li>communication/computation/memory effcient and scalable optimization algorithms</li>
										<li>collaborative learning (asynchronous, adversarial, local training, heterogeneity, etc.)</li>
										<li>model compression (knowledge distillation, pruning, sparse optimization, quantization)</li>
										<li>information theory (compression, encoding schemes, vector quantization)</li>
										<!-- <li></li> -->
									</ul>
									</p>
								</div>
							</section>

						<!-- Two -->
							<section id="two">
								<div class="container">
									<h2>Research</h2>
									<p>
										My research focuses on <em>optimization theory and algorithms for machine learning</em>, with an emphasis on efficiency, scalability, and the theoretical understanding of optimization methods. These methods are particularly relevant for large-scale machine learning training and federated learning. Driven by applications in machine learning, my publications appear primarily in leading machine learning conferences such as <em>NeurIPS, ICML, AISTATS</em>, and <em>ICLR</em>, as well as journals like <em>JMLR</em> and <em>TMLR</em>.
									</p>
									<p>
										I completed my Ph.D. in <em>real harmonic analysis</em>, a branch of mathematics that explores the relationship between functions or signals and their frequency domain representations. <a href="https://arxiv.org/abs/2202.08693">My thesis</a> investigated the convergence and divergence properties of certain convolution-type integral operators.

										In addition, I have done some research in <em>algebra</em>. During my undergraduate studies at YSU, I completed a research project on universal algebraic structures called dimonoids, which led to a publication in <em>Algebra and Discrete Mathematics</em>. Later, at KAUST, I worked on symbolic computation, specifically on developing computer algebra techniques for automating certain aspects of PDE analyses.
									</p>
									<p>
										For the complete list of my publications, please visit my <a href="https://scholar.google.com/citations?user=dJNwgT8AAAAJ&hl=en">Google Scholar</a> page.
									</p>
								</div>
							</section>

						<!-- Three -->
							<section id="three">
								<div class="container">
									<h2>News</h2>
									<div class="features">
										<article>
											<a class="image"><img src="images/LU.png" alt="" /></a>
											<div class="inner">
												<span>December 2025</span>
												<h4>New Position: Assistant Professor (UK Lecturer) at Lancaster University.</h4>
												<p>I'm excited to announce that I've started my new position as an Assistant Professor (UK Lecturer) in the <a href="https://www.lancaster.ac.uk/maths/">School of Mathematical Sciences</a> at <a href="https://www.lancaster.ac.uk">Lancaster University</a>, joining the <a href="https://www.lancaster.ac.uk/maths/research/mars/">MARS (Mathematics for AI in Real-world Systems)</a> section.<p/>
											</div>
										</article>
										<article>
											<a class="image"><img src="images/arxiv_logo.jpg" alt="" /></a>
											<div class="inner">
												<span>November 2025</span>
												<h4>New paper on arXiv.</h4>
												<p><a href="https://arxiv.org/abs/2510.18784">CAGE: Curvature-Aware Gradient Estimation For Accurate Quantization-Aware Training</a> - joint work with Soroush Tabesh, Andrei Panferov, Alexandra Volkova, Dan Alistarh.</p>
												<p>Abstract: <em>Despite significant work on low-bit quantization-aware training (QAT), there is still an accuracy gap between such techniques and native training. To address this, we introduce CAGE (Curvature-Aware Gradient Estimation), a new QAT method that augments the straight-through estimator (STE) gradient with a curvature-aware correction designed to counteract the loss increase induced by quantization. CAGE is derived from a multi-objective view of QAT that balances loss minimization with the quantization constraints, yielding a principled correction term that depends on local curvature information. On the theoretical side, we introduce the notion of Pareto-optimal solutions for quantized optimization, and establish that CAGE yields strong convergence guarantees in the smooth non-convex setting. In terms of implementation, our approach is optimizer-agnostic, but we provide a highly-efficient implementation that leverages Adam statistics. CAGE significantly improves upon the prior state-of-the-art methods in terms of accuracy, for similar computational cost: for QAT fine-tuning, it halves the compression accuracy loss relative to the prior best method, while for QAT pre-training of Llama models, its accuracy for 3-bit weights-and-activations (W3A3) matches the accuracy achieved at 4-bits (W4A4) with the prior best method. The official implementation can be found over <a href="https://github.com/IST-DASLab/CAGE">this https URL</a>.</em><p/>
											</div>
										</article>
										<article>
											<a class="image"><img src="images/arxiv_logo.jpg" alt="" /></a>
											<div class="inner">
												<span>October 2025</span>
												<h4>New paper on arXiv.</h4>
												<p><a href="https://arxiv.org/abs/2510.05361">MT-DAO: Multi-Timescale Distributed Adaptive Optimizers with Local Updates</a> - joint work with Alex Iacob, Andrej Jovanovic, Meghdad Kurmanji, Lorenzo Sani, Samuel Horváth, William F. Shen, Xinchi Qiu, Nicholas D. Lane.</p>
												<p>Abstract: <em>Training large models with distributed data parallelism (DDP) requires frequent communication of gradients across workers, which can saturate bandwidth. Infrequent communication strategies (e.g., Local SGD) reduce this overhead but, when applied to adaptive optimizers, often suffer a performance gap relative to fully synchronous DDP. We trace this gap to a time-scale mismatch: the optimizer's fast-moving momentum, tuned for frequent updates, decays too quickly to smooth gradients over long intervals, leading to noise-dominated optimization. To address this, we propose MT-DAO, a family of optimizers that employs multiple slow- and fast-moving first momenta or the gradient to track update dynamics across different time scales, for which we provide the first convergence guarantees. Empirically, for language-model pre-training, this eliminates the performance gap with DDP, outperforming infrequent-communication baselines in perplexity and reducing iso-token wall-clock time by 6-27% on Ethernet interconnects. At the 720M scale, MT-DAO reaches a target perplexity in 24% fewer steps and 35% less time than the single-momentum DDP baseline. MT-DAO enables effective cross-datacenter training and training over wide geographic areas.</em><p/>
											</div>
										</article>
										<article>
											<a class="image"><img src="images/NeurIPS_logo.png" alt="" /></a>
											<div class="inner">
												<span>September 2025</span>
												<h4>Paper accepted to NeurIPS 2025.</h4>
												<p><a href="https://arxiv.org/abs/2506.01863">Unified Scaling Laws for Compressed Representations</a> - joint work with Andrei Panferov, Alexandra Volkova, Ionut-Vlad Modoranu, Vage Egiazarian, Dan Alistarh.</p>
											</div>
										</article>
										<article>
											<a class="image"><img src="images/ISTA.png" alt="" /></a>
											<div class="inner">
												<span>June 2025</span>
												<h4>Featured in the <a href="https://d57372378d604d73ac4e014c1a11c0f2.marketingusercontent.com/m/view/g6qo2VUYmyGJXphnw2i6aeVUTgL1IBUbPXW9ne8lUEox#msdynttrid=cctOhPsRTgL0zJEI4LAArdugoQnVYLhBs1hG0ad7ooU">ISTA Campus Update newsletter</a>.</h4>
												<a class="image" style = "width: 95%;"><img src="images/ISTA-campus-update-me.png" alt="" /></a>
											</div>
										</article>
										<article>
											<a class="image"><img src="images/TMLR_logo.png" alt="" /></a>
											<div class="inner">
												<span>June 2025</span>
												<h4>Paper accepted to TMLR.</h4>
												<p><a href="https://arxiv.org/abs/2210.16402">GradSkip: Communication-Accelerated Local Gradient Methods with Better Computational Complexity</a> - joint work with Artavazd Maranjyan and Peter Richtárik.</p>
											</div>
										</article>
										<article>
											<a class="image"><img src="images/arxiv_logo.jpg" alt="" /></a>
											<div class="inner">
												<span>June 2025</span>
												<h4>New paper on arXiv.</h4>
												<p><a href="https://arxiv.org/abs/2506.01863">Unified Scaling Laws for Compressed Representations</a> - joint work with Andrei Panferov, Alexandra Volkova, Ionut-Vlad Modoranu, Vage Egiazarian, Dan Alistarh.</p>
												<p>Abstract: <em>Scaling laws have shaped recent advances in machine learning by enabling predictable scaling of model performance based on model size, computation, and data volume. Concurrently, the rise in computational cost for AI has motivated model compression techniques, notably quantization and sparsification, which have emerged to mitigate the steep computational demands associated with large-scale training and inference. This paper investigates the interplay between scaling laws and compression formats, exploring whether a unified scaling framework can accurately predict model performance when training occurs over various compressed representations, such as sparse, scalar-quantized, sparse-quantized or even vector-quantized formats. Our key contributions include validating a general scaling law formulation and showing that it is applicable both individually but also composably across compression types. Based on this, our main finding is demonstrating both theoretically and empirically that there exists a simple "capacity" metric -- based on the representation's ability to fit random Gaussian data -- which can robustly predict parameter efficiency across multiple compressed representations. On the practical side, we extend our formulation to directly compare the accuracy potential of different compressed formats, and to derive better algorithms for training over sparse-quantized formats.</em><p/>
											</div>
										</article>
										<article>
											<a class="image"><img src="images/arxiv_logo.jpg" alt="" /></a>
											<div class="inner">
												<span>May 2025</span>
												<h4>New paper on arXiv.</h4>
												<p><a href="https://arxiv.org/abs/2505.22549">DES-LOC: Desynced Low Communication Adaptive Optimizers for Training Foundation Models</a> - joint work with Alex Iacob, Lorenzo Sani, Paris Giampouras, Samuel Horváth, Andrej Jovanovic, Meghdad Kurmanji, Preslav Aleksandrov, William F. Shen, Xinchi Qiu, Nicholas D. Lane.</p>
												<p>Abstract: <em>Scaling foundation model training with Distributed Data Parallel (DDP) methods is bandwidth-limited. Existing infrequent communication methods like Local SGD were designed to synchronize only model parameters and cannot be trivially applied to adaptive optimizers due to additional optimizer states. Current approaches extending Local SGD either lack convergence guarantees or require synchronizing all optimizer states, tripling communication costs. We propose Desynced Low Communication Adaptive Optimizers (DES-LOC), a family of optimizers assigning independent synchronization periods to parameters and momenta, enabling lower communication costs while preserving convergence. Through extensive experiments on language models of up to 1.7B, we show that DES-LOC can communicate 170x less than DDP and 2x less than the previous state-of-the-art Local ADAM. Furthermore, unlike previous heuristic approaches, DES-LOC is suited for practical training scenarios prone to system failures. DES-LOC offers a scalable, bandwidth-efficient, and fault-tolerant solution for foundation model training.</em><p/>
											</div>
										</article>
										<article>
											<a class="image"><img src="images/arxiv_logo.jpg" alt="" /></a>
											<div class="inner">
												<span>May 2025</span>
												<h4>New paper on arXiv.</h4>
												<p><a href="https://arxiv.org/abs/2505.17967">SVD-Free Low-Rank Adaptive Gradient Optimization for Large Language Models</a> - joint work with Ionut-Vlad Modoranu, Erik Schultheis, Dan Alistarh.</p>
												<p>Abstract: <em>Low-rank optimization has emerged as a promising direction in training large language models (LLMs) to reduce the memory usage of adaptive optimizers by constraining learning to a lower-dimensional space. Prior work typically projects gradients of linear layers using approaches based on Singular Value Decomposition (SVD). However, applying SVD-based procedures individually to each layer in large models is computationally expensive and incurs additional memory costs due to storing the projection matrices. In this work, we propose a computationally efficient and conceptually simple two-step procedure to approximate SVD-based gradient projections into lower-dimensional spaces. First, we construct a complete orthogonal basis using predefined orthogonal matrices of the Discrete Cosine Transform (DCT). Second, we adaptively select basis columns based on their alignment with the gradient of each layer. Each projection matrix in our method is obtained via a single matrix multiplication followed by a lightweight sorting step to identify the most relevant basis vectors. Due to the predefined nature of the orthogonal bases, they are computed once at the start of training. During training, we store only the indices of the selected columns, avoiding the need to store full projection matrices for each layer. Our numerical experiments on both pre-training and fine-tuning tasks demonstrate the effectiveness of our dual strategy in approximating optimal low-rank projections, matching the performance of costly SVD-based methods while achieving faster runtime and reduced memory usage.</em><p/>
											</div>
										</article>
										<article>
											<a class="image"><img src="images/ICLR_logo.svg" alt="" /></a>
											<div class="inner">
												<span>January 2025</span>
												<h4>Paper accepted to ICLR 2025.</h4>
												<p><a href="https://arxiv.org/abs/2410.16103">LDAdam: Adaptive Optimization from Low-dimensional Gradient Statistics</a> - joint work with Thomas Robert, Ionut-Vlad Modoranu, Dan Alistarh.</p>
											</div>
										</article>
										<article>
											<a class="image"><img src="images/arxiv_logo.jpg" alt="" /></a>
											<div class="inner">
												<span>October 2024</span>
												<h4>New paper on arXiv.</h4>
												<p><a href="https://arxiv.org/abs/2410.16103">LDAdam: Adaptive Optimization from Low-dimensional Gradient Statistics</a> - joint work with Thomas Robert, Ionut-Vlad Modoranu, Dan Alistarh.</p>
												<p>Abstract: <em>We introduce LDAdam, a memory-efficient optimizer for training large models, that performs adaptive optimization steps within lower dimensional subspaces, while consistently exploring the full parameter space during training. This strategy keeps the optimizer's memory footprint to a fraction of the model size. LDAdam relies on a new projection-aware update rule for the optimizer states that allows for transitioning between subspaces, i.e., estimation of the statistics of the projected gradients. To mitigate the errors due to low-rank projection, LDAdam integrates a new generalized error feedback mechanism, which explicitly accounts for both gradient and optimizer state compression. We prove the convergence of LDAdam under standard assumptions, and show that LDAdam allows for accurate and efficient fine-tuning and pre-training of language models.</em><p/>
											</div>
										</article>
										<article>
											<a class="image"><img src="images/NeuralMagic_logo.jpg" alt="" /></a>
											<div class="inner">
												<span>October 2024</span>
												<h4>Secondment with Neural Magic, Inc.</h4>
												<p>As part of the fellowship, I have started 6-month secondment with <a href="https://neuralmagic.com/">Neural Magic, Inc.</a> in the USA, working with <a href="https://www.linkedin.com/in/marquesan?original_referer=https%3A%2F%2Fwww.google.com%2F">Dr. Alexandre Marques</a> on LLM quantization.</p>
											</div>
										</article>
										<article>
											<a class="image"><img src="images/NeurIPS_logo.png" alt="" /></a>
											<div class="inner">
												<span>September 2024</span>
												<h4>Two papers accepted to NeurIPS 2024.</h4>
												<p><a href="https://arxiv.org/abs/2405.15593">MicroAdam: Accurate Adaptive Optimization with Low Space Overhead and Provable Convergence</a> - joint work with Ionut-Vlad Modoranu, Grigory Malinovsky, Eldar Kurtic, Thomas Robert, Peter Richtárik, Dan Alistarh.</p>

												<p><a href="https://arxiv.org/abs/2408.17163">The Iterative Optimal Brain Surgeon: Faster Sparse Recovery by Leveraging Second-Order Information</a> - joint work with Diyuan Wu, Ionut-Vlad Modoranu, Denis Kuznedelev, Dan Alistarh.</p>
											</div>
										</article>
									</div>
								</div>
							</section>

						<!-- Four -->
							<section id="four">
								<div class="container">
									<h2>Contacts</h2>
									<div class="contact-item">
								        <h3>
								        	<a href="mailto:" class="icon solid fa-envelope"></a>
								            <h4><a href="mailto:">m.{lastname}@lancaster.ac.uk</a></h4>
								        </h3>
							        </div>
							        <div class="contact-item">
								        <h3>
								        	<a class="icon solid fas fa-map-marker-alt"></a>
								        	<h4>Fylde College, Bailrigg, Lancaster LA1 4YW, UK</h4>
								        </h3>
							        </div>
								</div>
							</section>

				<!-- Footer -->
					<section id="footer">
						<div class="container">
							<ul class="copyright">
								<li>&copy; Mher Safaryan. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
							</ul>
						</div>
					</section>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>