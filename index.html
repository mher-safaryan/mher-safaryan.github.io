<!DOCTYPE HTML>
<!--
	Read Only by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Mher Safaryan</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Header -->
			<section id="header">
				<header>
					<span class="image avatar"><img src="images/avatar.jpg" alt="" /></span>
					<h1 id="logo">Mher Safaryan</h1>
					<p>Marie Curie Fellow <br /> IST Austria</p>
				</header>
				<div>
					<ul class="icons">
						<li><a href="https://scholar.google.com/citations?user=dJNwgT8AAAAJ&hl=en" class="ai ai-google-scholar"><span class="label"></span></a></li>
						<li><a href="https://www.semanticscholar.org/author/M.-Safaryan/144033132" class="ai ai-semantic-scholar"><span class="label"></span></a></li>
						<li><a href="https://orcid.org/0000-0001-6290-1398" class="ai ai-orcid"><span class="label"></span></a></li>
						<li><a href="https://www.linkedin.com/in/mher-safaryan-94565a257/" class="icon brands fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
						<li><a href="images/cv.pdf" class="ai ai-cv"><span class="label"></span></a></li>
						<li><a href="mailto:mher.safaryan@ist.ac.at" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
					</ul>
				</div>
				<nav id="nav">
					<ul>
						<li><a href="#one" class="active">Bio</a></li>
						<li><a href="#two">Research</a></li>
						<li><a href="#three">News</a></li>
						<li><a href="#four">Contacts</a></li>
					</ul>
				</nav>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">

						<!-- One -->
							<section id="one">
								<!-- <div class="image main" data-position="center">
									<img src="images/banner.jpg" alt="" />
								</div> -->
								<div class="container">
									<header class="major">
										<h2>Bio</h2>
										<!-- <p>Just an incredibly simple responsive site<br />
										template freebie by <a href="http://html5up.net">HTML5 UP</a>.</p> -->
										<!-- <a href=""></a> -->
									</header>
									<p>
										I am a Marie Skłodowska-Curie fellow (MSCA COFUND IST-BRIDGE) at the <a href="https://www.ist.ac.at">Institute of Science and Technology Austria (ISTA)</a>, working in the group led by <a href="https://daslab.ista.ac.at/">Prof. Dan Alistarh</a>.
										<!-- As part of the fellowship, I am currently on secondment with <a href="https://neuralmagic.com/">Neural Magic, Inc.</a> in the USA, collaborating with <a href="https://www.linkedin.com/in/marquesan?original_referer=https%3A%2F%2Fwww.google.com%2F">Dr. Alexandre Marques</a>. -->
									<p/>
									<p>
										Before joining ISTA in 2022, I was a postdoctoral research fellow at <a href="https://cemse.kaust.edu.sa">King Abdullah University of Science and Technology (KAUST)</a> in Saudi Arabia from 2019 to 2022, in the group led by <a href="https://richtarik.org">Prof. Peter Richtárik</a>. Prior to that, I worked with <a href="https://academia.kaust.edu.sa/en/persons/diogo.gomes">Prof. Diogo Gomes</a> at KAUST as a research technician from 2016 to 2019.

										I obtained my Ph.D. in Mathematics in 2018 from <a href="http://ysu.am/main/en">Yerevan State University (YSU)</a> in Armenia, under the supervision of <a href="http://math.sci.am/user/grigori-karagulyan">Prof. Grigori Karagulyan</a>.
									</p>
									<p>
										<h3>Research Interests</h3>
									<ul class="research-interests">
										<li>optimization (theory and algorithms), machine learning, federated learning</li>
										<li>large-scale, convex/non-convex, stochastic/deterministic optimization, variance reduction</li>
										<li>communication/computation/memory effcient and scalable optimization algorithms</li>
										<li>collaborative learning (asynchronous, adversarial, local training, heterogeneity, etc.)</li>
										<li>model compression (knowledge distillation, pruning, sparse optimization, quantization)</li>
										<li>information theory (compression, encoding schemes, vector quantization)</li>
										<!-- <li></li> -->
									</ul>
									</p>
								</div>
							</section>

						<!-- Two -->
							<section id="two">
								<div class="container">
									<h2>Research</h2>
									<p>
										My current research focuses on <em>optimization theory and algorithms for machine learning</em>, with an emphasis on efficiency, scalability, and the theoretical understanding of optimization methods. These methods are particularly relevant for large-scale machine learning training and federated learning. Driven by applications in machine learning, my publications appear primarily in leading machine learning conferences such as <em>NeurIPS, ICML, AISTATS</em>, and <em>ICLR</em>, as well as journals like <em>JMLR</em> and <em>TMLR</em>.
									</p>
									<p>
										I completed my Ph.D. in <em>real harmonic analysis</em>, a branch of mathematics that explores the relationship between functions or signals and their frequency domain representations. My thesis investigated the convergence and divergence properties of certain convolution-type integral operators. I defended <a href="https://arxiv.org/abs/2202.08693">my thesis</a> in 2018, with a total of five journal publications, including two single-authored papers and two published in <em>The Journal of Geometric Analysis</em>.
									</p>
									<p>
										In addition, I have done some research in <em>algebra</em>. During my undergraduate studies at YSU, I completed a research project on universal algebraic structures called dimonoids, which led to a publication in <em>Algebra and Discrete Mathematics</em>. Later, at KAUST, I worked on symbolic computation, specifically on developing computer algebra techniques for automating certain aspects of PDE analyses.
									</p>
									<p>
										For the complete list of my publications, please visit my <a href="https://scholar.google.com/citations?user=dJNwgT8AAAAJ&hl=en">Google Scholar</a> page.
									</p>
								</div>
							</section>

						<!-- Three -->
							<section id="three">
								<div class="container">
									<h2>News</h2>
									<div class="features">
										<article>
											<a class="image"><img src="images/ICLR_logo.svg" alt="" /></a>
											<div class="inner">
												<span>January 2025</span>
												<h4>Paper accepted to ICLR 2025.</h4>
												<p><a href="https://arxiv.org/abs/2410.16103">LDAdam: Adaptive Optimization from Low-dimensional Gradient Statistics</a> - joint work with Thomas Robert, Ionut-Vlad Modoranu, Dan Alistarh.</p>
											</div>
										</article>
										<article>
											<a class="image"><img src="images/arxiv_logo.jpg" alt="" /></a>
											<div class="inner">
												<span>October 2024</span>
												<h4>New paper on arXiv.</h4>
												<p><a href="https://arxiv.org/abs/2410.16103">LDAdam: Adaptive Optimization from Low-dimensional Gradient Statistics</a> - joint work with Thomas Robert, Ionut-Vlad Modoranu, Dan Alistarh.</p>
												<p>Abstract: <em>We introduce LDAdam, a memory-efficient optimizer for training large models, that performs adaptive optimization steps within lower dimensional subspaces, while consistently exploring the full parameter space during training. This strategy keeps the optimizer's memory footprint to a fraction of the model size. LDAdam relies on a new projection-aware update rule for the optimizer states that allows for transitioning between subspaces, i.e., estimation of the statistics of the projected gradients. To mitigate the errors due to low-rank projection, LDAdam integrates a new generalized error feedback mechanism, which explicitly accounts for both gradient and optimizer state compression. We prove the convergence of LDAdam under standard assumptions, and show that LDAdam allows for accurate and efficient fine-tuning and pre-training of language models.</em><p/>
											</div>
										</article>
										<article>
											<a class="image"><img src="images/NeuralMagic_logo.jpg" alt="" /></a>
											<div class="inner">
												<span>October 2024</span>
												<h4>Secondment with Neural Magic, Inc.</h4>
												<p>As part of the fellowship, I have started my secondment with <a href="https://neuralmagic.com/">Neural Magic, Inc.</a> in the USA, working with <a href="https://www.linkedin.com/in/marquesan?original_referer=https%3A%2F%2Fwww.google.com%2F">Dr. Alexandre Marques</a> on LLM compression.</p>
											</div>
										</article>
										<article>
											<a class="image"><img src="images/NeurIPS_logo.png" alt="" /></a>
											<div class="inner">
												<span>September 2024</span>
												<h4>Two papers accepted to NeurIPS 2024.</h4>
												<p><a href="https://arxiv.org/abs/2405.15593">MicroAdam: Accurate Adaptive Optimization with Low Space Overhead and Provable Convergence</a> - joint work with Ionut-Vlad Modoranu, Grigory Malinovsky, Eldar Kurtic, Thomas Robert, Peter Richtarik, Dan Alistarh.</p>

												<p><a href="https://arxiv.org/abs/2408.17163">The Iterative Optimal Brain Surgeon: Faster Sparse Recovery by Leveraging Second-Order Information</a> - joint work with Diyuan Wu, Ionut-Vlad Modoranu, Denis Kuznedelev, Dan Alistarh.</p>
											</div>
										</article>
									</div>
								</div>
							</section>

						<!-- Four -->
							<section id="four">
								<div class="container">
									<h2>Contacts</h2>
									<div class="contact-item">
								        <h3>
								        	<a href="mailto:mher.safaryan@ist.ac.at" class="icon solid fa-envelope"></a>
								            <h4><a href="mailto:mher.safaryan@ist.ac.at">mher.safaryan@ist.ac.at</a></h4>
								        </h3>
							        </div>
							        <div class="contact-item">
								        <h3>
								        	<a class="icon solid fas fa-map-marker-alt"></a>
								        	<h4>Building West, Level 1, 21-01-122, ISTA, Am Campus 1, 3400 Klosterneuburg, Austria</h4>
								        </h3>
							        </div>
								</div>
							</section>

				<!-- Footer -->
					<section id="footer">
						<div class="container">
							<ul class="copyright">
								<li>&copy; Mher Safaryan. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
							</ul>
						</div>
					</section>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>